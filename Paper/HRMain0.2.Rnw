%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Interpreting Hazard Ratios in Political Science
% Christopher Gandrud
% 27 February 2013
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper]{article}
\usepackage{fullpage}
\usepackage[authoryear]{natbib}
\usepackage{setspace}
    \doublespacing
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=cyan,
    urlcolor=cyan
}
\usepackage{dcolumn}
\usepackage{tikz}
\usepackage{todonotes}

<<include=FALSE>>=
#### Load Packages ####
library(repmis)

Packages <- c("car", "knitr", "repmis", "devtools", 
              "simPH", "survival", "Zelig", "ggplot2")

LoadandCite(Packages, file = "HRPackages.bib")

#### Load data ####
# Load Carpenter (2002) data 
## The data is included with simPH
data("CarpenterFdaData")

# Load Golub & Steunenberg (2007) data
## The data is included with simPH
data("GolubEUPData")

##### Set Chunk Options ####
opts_chunk$set(fig.align='center')
@

\begin{document}

% Title
\title{Putting Data Back in Time: Getting the Most Out of Hazard Ratios in Diffusion Research}
    \author{Christopher Gandrud\footnote{Lecturer in International Relations (\href{mailto:gandrud@yonsei.ac.kr}{gandrud@yonsei.ac.kr}) Thank you to Jeffrey Chwieroth. Note: I wrote the paper with \texttt{knitr} \citep{R-knitr}. The paper, including all empirical analyses and graphs can be reproduced with the source code files available at: \url{https://github.com/christophergandrud/InterpretingHazRatios}.}}

\maketitle

% Abstract
\begin{abstract}

\noindent\emph{Very early draft version. Comments welcome.} \\[0.2cm]

Recently the Cox Proportional Hazards (PH) model has become a popular method in political science for examining cross-unit cross-time data. However, as \cite{Licht2011}, \cite{Keele2010}, and \cite{Hern'an2010} have pointed out many researchers using EHA have inadequately addressed issues of model misspecification. This is unfortunate because causes of model misspecification, particularly non-linearity and time-varying effects may be substantively relevant for political science researchers. Ignoring them means that researchers are not using EHA to its full potential. For example, focusing on time averaged effects ignores EHA's ability to examine how time-period specific events--such as global shocks or the promotion of new policy ideas--interact with traditional covariates. In this article I combine advocate a strategy for political scientists using Cox PH models that addresses these issues. Graphically presenting results can be a useful way of overcoming these problems so I introduce a new R package--\emph{simPH}--that includes tools for graphically exploring time-varying and non-linear Cox PH estimates along with estimation uncertainty. I illustrate \emph{simPH}'s capabilities with data from recent political science research.

\end{abstract}

\begin{description}
  \item [{\textbf{Keywords:}}] Cox Proportional Hazard models, Event History Analysis, hazard ratios, time-varying, non-linearity
\end{description}

\vspace{0.3cm}

Recent articles in \emph{Political Analysis} have highlighted two frequently overlooked sources of model misspecification and misinterpretation in the popular Cox Proportional Hazards (PH) model \citep{cox1972}. \cite{Licht2011} discusses how to model and interpret covariate effects that violate the proportional hazards assumption. \cite{Keele2010} points out that un-modeled non-linear relationships in Cox PH can bias results. In addition, \cite{Hern'an2010} recently published a short article in \emph{Epidemiology} entitled ``The Hazard of Hazard Ratios". He highlighted two common problems with the way that hazard ratios (HR) are often interpreted. He argued that researchers ignore (1) how hazard ratios  change over time and (2) the fact that their studies' observation periods have ``built-in selection-biases". Though he was focusing on epidemiological research, time-period specific biases are a potentially important issue for political scientists to address, especially when interpreting time-varying effects. 

The Cox PH model and Event History Analysis (EHA) in general is increasingly being used to address political science questions. A small sample of recent studies using Cox PH and EHA includes: \cite{Aleman2011, BuenodeMesquita1999, brooks2005, Gandrud2012, Gates2006, Golub2007, Jordana2005, Mccubbins2009, Neumayer2002, Simmons2006}. As the Cox PH model has become more prevalent in political science we need better strategies and tools for avoiding the problems that \cite{Licht2011}, \cite{Keele2010}, and \cite{Hern'an2010} identify.

My aim in this paper is to lay out a strategy and computational tools for addressing these problems. First, I briefly summarize Licht, Keele, and Hern\'{a}n's arguments. Second, I combine them together into a single strategy for political scientists using Cox PH models. Graphically presenting results can be a useful way of addressing these issues so I introduce a new R \citep{RLanguage} package--\emph{simPH} \citep{R-simPH}--that includes tools for graphically exploring time-varying and non-linear Cox PH estimates along with estimation uncertainty. I illustrate \emph{simPH}'s capabilities with data from recent political science research.

%%%%%% Section: Common issues
\section{Common issues in analyses with Cox PH models}

In this paper I focus on one of the more commonly used EHA models in both political science, the Cox Proportional Hazard Model \citet{cox1972}.\footnote{In general my conclusions also apply to Fine and Gray's \citeyearpar{Fine1999} Competing Risks Model which is a Cox PH analogue for competing risks EHA.} The Cox PH is a semi-parametric EHA model that allows researchers to examine how specified factors influence the rate of a particular event happening--a policy is adopted, a government falls, or a war breaks out--at a particular point in time given that the even has not already occurred.

This rate is commonly referred to as the hazard rate ($h_{i}(t)$).\footnote{It is given formally by: $h_{i}(t) = \lim\limits_{\Delta t \rightarrow 0}\frac{\mathrm{Pr}(t \leq T < t + \Delta t | T \leq t)}{\Delta t}$, where $T$ is is the time that an event occurred over the interval $[t,\:\Delta t]$.} The hazard rate for unit $i$ at time $t$ is estimated with the Cox PH model using: 
%
\begin{equation}
    h(t|\mathbf{X}_{i})=h_{0}(t)\mathrm{e}^{(\mathbf{\beta X}_{i})},
\end{equation}
%
where $h_{0}(t)$ is the baseline hazard, i.e. the instantaneous rate of a transition at time $t$ when all of the covariates are zero. $\mathbf{\beta}$ is a vector of coefficients and $\mathbf{X}_{i}$ is the vector of covariates for unit $i$.

We are usually interested in how a covariate changes the rate of an event happening. For example, does having a majoritarian electoral system increase the probability of a country adopting a given policy? In general researchers have tried to answer these questions by looking at the Cox PH coefficient estimates $\beta$.\footnote{Like logistic regression, the coefficient is more easily interpreted if we examine its exponent $\exp(\beta)$, i.e. the hazard ratio (HR). Using the HR we can find the predicted percentage change in the hazard rate when we compare two values of a given variable. To find how the effect on the hazard rate of some value $X_{1}$ compared to $X_{2}$ with a coefficient $\beta$ we use the following formula from \citet[60]{boxsteffensmeier2004}: $\%\triangle h(t) = \left[\frac{\mathrm{e}^{(\beta X_{1})} - \mathrm{e}^{(\beta X_{2})}} {\mathrm{e}^{(\beta X_{2})}}\right].$} However, only examining single coefficients can lead to significant model misspecification and erroneous substantive interpretation of Cox PH results.

\subsection{Non-proportional hazards}

One of the most important sources of estimation bias in Cox PH models discussed at length by \cite{Licht2011}, \cite{BoxSteffensmeier2001}, and \cite{boxsteffensmeier2004} is a violation of the proportional hazards assumption (PHA). The PHA is that the hazards of two units experiencing an event are proportional to one another and that this relationship is constant overtime. Formally, for the PHA to hold the hazard for units $j$ and $l$ then:\footnote{This is also the equation for the hazard ratio between $X_{j}$ and $X_{l}$.}
%
\begin{equation}
	\frac{h_{j}(t)}{h_{l}(t)} = \mathrm{e}^{\beta\prime(X_{j} - X_{l})}.
\end{equation}
%
If the PHA is violated and measures are not taken to correct for the violation, then researchers may obtain biased parameter estimates and statistical tests with lower power \citep{Therneau1990,Keele2010}. Beyond these statistical problems, not adjusting for violations of the PHA can obscure phenomenon that political researchers are particularly interested in finding.

\cite{Licht2011} identifies a number of important political science theories that actually lead us to expect that the PHA will be violated. She says ``the nature of the political processes of learning, institutionalization, strategic developments, and information transmission \ldots are likely to produce frequent violations of the PHA'' \citeyearpar[228]{Licht2011}. In particular, building on \cite{Finnemore1998} and \cite{blyth1997} researchers have attempted to empirically examine how ideas may affect policy change and diffusion relative to geopolitical and domestic political and economic factors, among others. An important empirically observable difference between these types of theories is how they predict the effects of specific factors \emph{will change over time relative to others}.\footnote{Note that the effects of variables in Cox PH can change overtime without violating the PHA via a change in the baseline hazard. We will look at this issue in more detail later in this section.} For example, the effect of norms and ideational diffusion mechanisms \citep[see][]{Yee1996} are predicted to strengthen overtime. Whereas the effect of most domestic political and economic and geopolitical factors are expected to have a constant or perhaps decreasing effect overtime \citep[228]{Licht2011}. It is likely that more than one of these approaches has an effect on any given political event of interest. Therefore we should expect that some of our covariates will violate the PHA because their effects on the hazard of an event change non-proportionally over time relative to one another. Properly modeling how covariate effects change overtime is crucial for 	adequately testing our hypotheses. 

There are a number of widely used tests to examine whether or not the PHA is violated. See \cite{Grambsch1994}, \cite{BoxSteffensmeier2001}, and \cite{boxsteffensmeier2004} for discussions of various methods of testing the PHA.\footnote{Many common software packages implement versions of these tests. R's \emph{survival} package \citep{R-survival} implements Grambsch and Therneau's \citeyearpar{Grambsch1994} modified Schoenfeld residuals test. This is done with the \texttt{cox.zph} command.} If a covariate is determined to violate the PHA, Box-Steffensmeier and co-authors \citep[see][]{BoxSteffensmeier2003,boxsteffensmeier2004} suggest directly modeling the relationship between the variable and time. This usually entails including an interaction between the variable and some function of time such as the natural logarithm or some exponent.\footnote{The decision to use a particular functional form should be guided by theory and will likely also be influenced by findings in the data.} If $f(t)$ is some function of time then a simple Cox PH model estimating the hazard rate for unit $i$ with one time-interaction is given by:
%
\begin{equation}
	h_{i}(t|\mathbf{X}_{i})=h_{0}(t)\mathrm{e}^{(\beta_{1}X_{i} + \beta_{2}f(t)X_{i})}.
\end{equation}

Like any other interaction effect \cite[see][]{Brambor2006} extra care should be taken when interpreting the $\beta_{1}$ and $\beta{2}$ parameter estimates. We cannot simply interpret the effect $X$ by looking at $\beta_{1}$ or $\beta_{2}$ in isolation. They need to be combined. Licht's argues that post-estimation simulation techniques should be employed to substantively interpret these combined coefficients and the uncertainty surrounding them. 

She describes two methods of calculating the combined effect of a time interaction in ways that are relatively easy to interpret: (a) first differences and (b) relative hazards. A first difference is the percentage change in the hazard rate at time ($t$) between two given values of $X$:
%
\begin{equation}
	\%\triangle h_{i}(t) = (e^{(X_{j} - X_{l})(\beta_{1} + \beta{2}f(t))} - 1) * 100.
\end{equation}
% 
Relative hazards\footnote{The term was advanced by \cite{Golub2007}.} are given by:
%
\begin{equation}
	\frac{h_{j}(t)}{h_{l}(t)} = \mathrm{e}^{X_{j}(\beta_{1} + \beta_{2}f(t))},
\end{equation}
%
In this situation the covariate $X_{l}$ is 0. Relative hazards represent the change in the hazard when $X$ is ``switched on'' as it would be when comparing an European Union member to a non-member, for example.

Relative hazards are particularly useful for binary variables because they reflect the change in the hazards from $X = 0$ to $X = 1$. The first difference can be more useful for continuous variables as changes in $X$ from 0 to 1 may be inconsequential or even impossible when we are looking at, for example, the effect of GDP per Capita. In these cases the first difference will be more appropriate. Though the choice between relative hazard and first difference can be cosmetic. They are virtually the same if $X_{j} - X_{l} = 1$.\footnote{Of course first differences are percentage changes.}

Following \cite{King2000}, Licht argues that post-estimation simulation techniques make it easy to estimate the uncertainty surrounding the first difference and relative hazards. In both cases we first find the the parameter estimates for $\hat{\beta_{1}}$ and $\hat{\beta_{2}}$ from a Cox PH model as well as the variance matrix $\hat{V}(\hat{\beta})$. Second, we draw $n$ values of $\beta_{1}$ and $\beta_{2}$ from the multivariate normal distribution given by $\hat{\beta} \sim N(\hat{\beta}, \hat{V}(\hat{\beta}))$. Third, we use these simulated values to calculate either the first difference or relative hazard for a range of times $T = [t_{1}\ldots t_{T}]$ as well as specified values of $X_{j}$ and $X_{l}$ (as appropriate). Using these simulation techniques allows us to estimate the full time-varying effect, how it changes over time, substantively asses the effect, and show the uncertainty surrounding our estimates.
 
\subsection{Non-linear Effects}

Time-varying coefficient effects are not the only cause of PHA violations. Building on \cite{Grambsch1994} and \cite{Therneau2000}, \cite{Keele2010}\footnote{See also \cite{Keele2008} chapter 6.} points out that common diagnostic tests will as indicate PHA violations if the model is misspecified for other reasons. Omitting an important covariate, using a proportional hazards model even if another EHA model is more appropriate, or including a covariate as linear when its effect is actually non-linear can lead to significant PHA violation tests. 

Because of this Keele suggests that \emph{before} testing the PHA we should (a) try to make sure that we are not omitting important variables, (b) find the appropriate functional form for the covariates, typically either polynomials or splines. Many studies using Cox PH models have not test for nonlinearity, but instead jumped straight to testing the PHA, including time-interactions when it was violated. As Keele \citeyearpar{Keele2010} demonstrates ascribing a time-varying effect to a covariate when in fact the effect varies not over time, but non-linearly over values of the covariate can have major implications for substantive interpretation of results from Cox PH models.

After omitted variables and non-linearities have been addressed, then (c) we should test for the PHA. If it is violated we can add time interactions as before. If after doing all of these steps non-proportional hazards are still present then (d) he suggests we should look for an EHA model, such as the log-logistic model--that does not assume proportional hazards.

\subsection{Time-period specific biases}

\todo[inline]{Haven't looked at the baseline. Effects common to all units.}

The last major issue that can affect our interpretation of Cox PH results involves the interpretation of hazard ratios, regardless of whether the effect is non-linear, time-varying, or not, without reference to time. \cite{Hern'an2010} highlights two problems with interpreting hazard ratio's without reference to time. These are:

\begin{description}\label{Marginal}
  \item [{\textbf{Problem 1:}}] Hazard ratios are averaged over the study's observation period;
  \item [{\textbf{Problem 2:}}] Time period-specific HRs have an inherent selection-bias. 
\end{description}

Starting with Problem 1, hazard ratios are the estimated effects of variables on the hazard rate averaged over a study's particular observation period. Equation 2 gives us a variable's time-averaged marginal effect. The value of time-averaged HRs depend on a study's duration and be influenced by the length of the study. As with any mean value, HRs may in fact be large at one point in the study and then small in another. Presenting just time-averaged HRs does not reveal these trends. Importantly, this problem can arise \emph{even if the proportional hazards assumption is not violated}.     

Let's consider how this works in Cox Proportional Hazard models. Cox PH does not have an intercept. It does have a baseline hazard $h_{0}(t)$. Note that baseline hazards change at each point in time ($t$). So we need to consider the effective `interaction' between both hazard ratios and the baseline hazard at each point in time to understand what is going on. Methods of presenting these results included plotting survival curves and hazard rates using fitted values over our studies' observation periods. These graphs give us a sense of the variables' effects at each point in time.

If hazard ratios can change at each point in time, then a study's specific observation period can effect our results. Hern\'{a}n largely focuses on what I call \emph{duration period-specific selection-bias} (PSSB). 

For example, imagine that we are interested in whether or not a country adopted a policy P ($\mathrm{P} = 1$). We create a simple EHA with one independent variable $X$, such as democracy. We test the model on twenty years worth of data. Imagine that the averaged hazard ratio from this model is positive--higher values of $X$ increase the hazard of adopting policy P. When we look at the HR over time we find that in the first ten years of the observation period the HR is positive then becomes weakly negative for the last decade. 

What causal story could explain this? Duration PSSB may be at work. There could be a proportion of countries in our sample that are more likely to adopt policy P when they also have a positive value of $X$, because $X$ is interacting with some unobserved factor Z to produce a common effect C. Using epidemiological terminology, these countries are more `susceptible'. Over the course of the observation period, the more susceptible countries adopt policy P at a higher rate than countries that just have high values of $X$. After the susceptible countries exit the risk set, we are left with a sample of less susceptible countries. $X$ in the absence of Z may actually have a slight negative causal effect, which we observe after the susceptible countries drop out of our sample. 


%%%%%% Section: Modeling Strategy
\section{A Cox PH modeling strategy}

We can combine these suggestions into one research strategy for those setting out to examine a phenomenon with Cox PH models. It has five steps:

\begin{enumerate}
	\item Determine the covariates and covariate interactions to include in the model.
	\item Test for non-linearity using splines and polynomials.
	\item Test for violations of the proportional hazards assumption.
	\item If non-proportionality exists include time interactions. If not, consider common time-period specific biases.
	\item If non-proportionality still exists after including time interactions, consider another model that does not use the PHA. Continue to test for common time-period specific biases.
	\item If there is no longer evidence of non-proportionality examine common time-period specific biases.
\end{enumerate}

%%%%%% Section: simPH
\section{\emph{simPH}: tools for simulating and graphing effects from Cox PH models}

One reason that researchers have inconsistently followed these steps it that there as been a lack of computational capabilities to implement them. In R the \emph{survival} and \emph{Zelig} \citep{R-zelig} packages have capabilities for estimating splines and time interactions. However, their functions for graphically showing results, especially over time, has been limited. Capabilites for showing results from penalised splines\footnote{Primarily the \texttt{termplot} command. See \cite{Keele2008}, chapter 6.} do not simulate uncertainty in the way described earlier from Cox PH models. They also do not show how effects are estimated to change over time. \emph{Zelig} can simulate and graph parameter values, but not for combined interactions. In general the capabilities for showing results with time interactions is very limited. Usually, showing these types of results requires considerable researcher effort to extract estimates from model objects and then devise a way to show them graphically. See for example Licht's code for replicating the time-interaction plots in her paper.\footnote{It is available at: \url{http://hdl.handle.net/1902.1/15633}.}

To solve these problems I am introducing the \emph{simPH} package for R.\footnote{See the Appendix for installation instructions.} There are three basic steps to use the package:

\begin{enumerate}
	\item Estimate a Cox PH model using \emph{survival}'s \texttt{coxph} command,
	\item Simulate parameter estimates and calculate the relative hazards, first differences, or hazard ratios for a given parameter using the \emph{simPH} command corresponding to the variable type.\footnote{\texttt{coxsimLinear} can be used for linear, time constant variables, \texttt{coxsimInteract} for linear multiplicative interactions, \texttt{coxsimPoly} for polynomials, \texttt{coxsimSpline} for penalised splines, and \texttt{coxsimtvc} for time-varying coefficients.}
	\item Plot the simulations using the appropriate \emph{simPH} plotting command.\footnote{Commands include: \texttt{gglinear}, \texttt{gginteract}, \texttt{ggpoly}, \texttt{ggspline}, and \texttt{ggtvc}. These commands use \emph{ggplot2} \citep{R-ggplot2} and in some cases \texttt{scatter3d} from the \emph{car} packages \citep{R-car} to plot the simulations. You can use the simulation objects created by \emph{simPH} to make a graph with any other plotting package.} 
\end{enumerate}

\noindent The simulation functions follow \cite{King2000} (discussed above) to simulate and calculate a variety of quantities of interest. The user can specify the number of simulations to run. The more simulations we conduct,\footnote{The \emph{simPH} default is 1,000.} the better picture we get of the probability distributions our parameters are from \citep[349]{King2000}.\footnote{Note that in some cases, such as with hazard rates with penalised splines, it is easy to ask the program to create more simulations than average desktop computers can easily handle. Therefore the user may need to balance a a desire for a clear view of the probability distribution a quantity of interest comes from with what is computationally possible. } The plot functions then take these simulated values and plot them along with a smoothing line specified by the user to summarize their central tendency.

The functions in this package directly complement the strategy discussed in the previous section. They make it easy to interpret and present results without resorting to ``train schedules'' of coefficients and standard errors. Given that almost all of the quantities of interest, even hazard rates for single linear covariates, involve some interaction, the ``train schedule'' approach is a highly inadequate way to present results \citep{Brambor2006}. 

Most of the functions are capable of simulating and plotting hazard rates for multiple strata, i.e. when the baseline hazard is allowed to vary across different groups \cite[see][]{BoxSteffensmeier2006}.\footnote{Currently \emph{simPH} does not present hazard rates for penalised splines from stratified models. This is mostly because of the difficulty presenting 4 dimensional results (i.e. time, hazard rate, fitted values of the variable, and strata.)} A stratified Cox PH model is given by
%
\begin{equation}
   h(t|\mathbf{X}_{i})=h_{k0}(t)\mathrm{e}^{(\mathbf{\beta X}_{i})},
\end{equation}
%
where $k$ is a `group'.\footnote{Currently, \emph{simPH} does not simulate uncertainty for the baseline hazard parameter estimate. Hopefully this will be included in future versions.} Groups can be defined in a variety of ways. One common use of strata is to account for repeated event dependence \citep{BoxSteffensmeier2006}. An example of examining time period specific biases in repeated event situations is whether or not a time-period specific idea may effect units that have already implemented one type of a policy in the past compared to units that have never had a similar policy. So, stratified Cox PH models are particularly useful when examining time-period specific biases as it allows researchers to see if a common time period specific phenomenon effects different groups differently. 

%%%%%% Section: Demonstrations
\section{Demonstrations}

To illustrate how \emph{simPH} can be used by political scientists, I will use it to replicate key figures and findings from \cite{Keele2010}, \cite{Licht2011}, and \cite{Gandrud2012}.

\subsection{Non-linear effects}

To demonstrate why researchers need to check for and explicitly model non-linearity \cite{Keele2010} indirectly replicated (among others) a study by \cite{Carpenter2002} on the time it takes the US Food and Drug Administration (FDA) to approve a new drug. He added spline fits to the variables in one of Carpenter's models. Then he used a Wald nonlinearity test to examine whether the model with splines had a better fit than the linear model. He then ran a Grambsch and Therneau test of non-proportionality for both the model with linear coefficients and splines. He found evidence that four of the variables in the linear coefficient model violated the test, where as non of them violated the test in the spline model. This indicates that modeling non-linearity rather than time-varying effects is the more appropriate strategy. So, finally he built a model that kept the spline fits for the variables the Wald test indicated had a non-linear relationship and reintroduced the linear terms for the others. This allows him to draw conclusions like the number of FDA review staff increases the likelihood that a drug will be accepted, but that the effect diminishes after a threshold number of staff. Also, counter to Carpenter's findings, the number of groups representing a disease has little effect on whether or not a drug treating it is approved once nonlinearities in the model are accounted for. 

Previously it has been difficult to examine and communicate functional form, magnitude, and uncertainty surrounding spline effects. Tables are very cumbersome as a spline fitted effect effectively consists of multiple coefficients and standard errors that estimate the effect of values of a variable in a given range\footnote{The range is demarcated by ``knots''.}
on the hazard. Depending on the researcher determines size of the ranges\footnote{With R's \texttt{pspline} command the range is effectively adjusted by changing the spline's degrees of freedom.} there can quickly be many more coefficients than can be efficiently presented in a table and understood by a reader. In his results tables, Keele does not show spline coefficients and simply denotes their overall significance with the standard stars. In R you can plot--and Keele alludes having done this--the estimated spline effect over a range of values.\footnote{Again, this is done with the \texttt{termplot} command.} These plots, however, have a number of drawbacks. Firstly, the plots show the log hazard [IS THIS CORRECT?],\todo{Relate log hazard to hazard ratio.} which is not a particularly intuitive quantity to understand. It also plots standard errors, rather than the more widely used confidence intervals. Casual readers could easily think the uncertainty about the spline estimates is smaller than it really is.\footnote{Remember that the 95 percent confidence interval $CI$ for some point estimate $\hat{\beta}$ and standard error $SE$ is generally found with: $CI = \hat{\beta} \pm 1.96*SE$.} 

\emph{simPH} allows us to estimate quantities that we are more interested in like the relative hazards, hazard rates over time, as well as hazard ratios and first difference between different values of a variable for spline fitted effects.\footnote{If you would prefer to model non-linearities with polynomials, it also supports this. Though currently, it only supports relative hazards for polynomials.} For example, lets simulate the hazard ratios for the effect of the number of drug review staff on the time it takes for a drug to be approved. Figure \ref{Spline1} shows the simulated hazard ratios over the full range of FDA staff per drug trial observed in Carpenter's data.\footnote{The hazard ratios are for where $X$--FDA staff--are one unit apart.} Not only is this more informative than simply showing you the 12 coefficients and their standard errors that comprise the spline parameter estimates, it is also more informative than the current plotting alternatives in R, namely \texttt{termplot}. 

<<FitKeeleModel, include=FALSE>>=
# Run basic model
# From Keele (2010) replication data. Used to create Table 7.
M1 <- coxph(Surv(acttime, censor) ~  prevgenx + lethal + deathrt1 +
              acutediz + hosp01 + pspline(hospdisc, df = 4) + 
              pspline(hhosleng, df = 4) + mandiz01 + 
              femdiz01 + peddiz01 + orphdum + natreg + vandavg3 + 
              wpnoavg3 + pspline(condavg3, df = 4) + 
              pspline(orderent, df = 4) + pspline(stafcder, df = 4), 
              data = CarpenterFdaData)
@

\begin{figure}
  \caption{Simulated Hazard Ratios for the Effect of FDA Staff on Drug Approval Time}
  \label{Spline1}
<<Spline1Fig, echo=FALSE, message=FALSE, out.width="9cm", out.height="8cm", cache=TRUE>>=
# Simulated Fitted Values
Sim1 <- coxsimSpline(M1, bspline = "pspline(stafcder, df = 4)", 
                     bdata = CarpenterFdaData$stafcder,
                     qi = "Hazard Ratio",
                     Xj = seq(1100, 1700, by = 10), 
                     Xl = seq(1099, 1699, by = 10), ci = "90")

# Plot simulated values
ggspline(Sim1, qi = "Hazard Ratio", 
         xlab = "\n Number of FDA Drug Review Staff", palpha = 0.2)
@  

{\scriptsize {The figure's points show the middle 90 percent of the simulations at each value of FDA staff. \\ The line summarizing the central tendency of the distribution was created with a generalized additive model for integrated smoothness estimation.}}
\end{figure}

\begin{figure}
  \caption{Log Hazard for the Effect of FDA Staff on Drug Approval Time}
  \label{TermPlot}
<<Termplot, echo=FALSE, out.width="9cm", out.height="8cm", cache=TRUE>>=
# Create termplot for stafcder
termplot(M1, term = 17, se = TRUE, rug = TRUE, 
         ylab = "Log Hazard", xlab = "Number of FDA Drug Review Staff")
@
{\scriptsize {Dashed lines indicate the standard error.}}
\end{figure}

Figure \ref{TermPlot} uses \texttt{termplot} to plot the log hazard of the effect and the standard errors. Apart from showing related but different quantities of interest, Figure \ref{TermPlot} gives us less useful information than Figure \ref{Spline1} about the probability distribution that the parameter is from. By only describing the distribution of the parameter with the mean value and standard error, Figure \ref{TermPlot} makes it difficult for us to know what the distribution is. Figure \ref{Spline1} directly simulates this distribution and shows it directly.

\subsection{Time-varying effects}

Though current capabilities for showing quantities of interest for splines are marginally improved by the simulation capabilities of \emph{simPH}, there are currently no easy to use ways to visually explore time-varying effects. so \emph{simPH} substantially improves researchers' abilities to show these results. To demonstrate this, I will recreate plots from \cite{Licht2011}.\footnote{As I mentioned earlier, she makes the Stata source code available. \emph{simPH} largely makes it much easier to implement these methods.} 

She re-examines Golub and Steunenberg's \citeyearpar{Golub2007} analysis of EU directive deliberation. Figure \ref{TVCMV} recreates her figure \citeyearpar[][236]{Licht2011} showing the first difference of how the effect of qualified majority voting (QMV) legislative deliberation time changes as the number of days of deliberation increases.\footnote{Her original figure was not in terms of a percentage difference to make it more comparable to a figure in Golub and Steunenberg's original. I also have not separated out the pre and post Single European Act time periods for simplicity.} We can clearly see that using QMV increases the probability of passing a directive, early in the deliberation process (almost by 300 percent at first). But the deliberation process goes on further the effect decreases and then becomes negative at about 1,000 days. The code for how to recreate this graph is in the appendix. Needless to say it only requires two lines of code. 

\begin{figure}
  \caption{Simulated Relative Hazards for the Effect of Qualified Majority Voting on the Time it Takes to Pass Legislation.}
  \label{TVCQMV}
<<TVCqmv, echo=FALSE, message=FALSE, out.width="9cm", out.height="8cm", cache=TRUE>>=
# Create natural log time interactions
Golubtvc <- function(x){
  assign(paste0("l", x), tvc(GolubEUPData, b = x, tvar = "end", tfun = "log"))
}
GolubEUPData$Lcoop <-Golubtvc("coop")
GolubEUPData$Lqmv <- Golubtvc("qmv")
GolubEUPData$Lbacklog <- Golubtvc("backlog")
GolubEUPData$Lcodec <- Golubtvc("codec")
GolubEUPData$Lqmvpostsea <- Golubtvc("qmvpostsea")
GolubEUPData$Lthatcher <- Golubtvc("thatcher")

# Run Cox PH Model
M2 <- coxph(Surv(begin, end, event) ~
            qmv + qmvpostsea + qmvpostteu +
            coop + codec + eu9 + eu10 + eu12 +
            eu15 + thatcher + agenda + backlog +
            Lqmv + Lqmvpostsea + Lcoop + Lcodec +
            Lthatcher + Lbacklog,
         data = GolubEUPData,
         ties = "efron")

# Create simtvc object for Relative Hazard
Sim2 <- coxsimtvc(obj = M2, b = "qmv", btvc = "Lqmv",
                   qi = "First Difference",
                   tfun = "log", from = 80, to = 2000,
                   by = 15, ci = "95")

# Create relative hazard plot
ggtvc(Sim2, qi = "First Difference", xlab = "\nTime in Days")
@
{\scriptsize {The figure's points show the middle 95 percent of the simulations at each point in time. \\ The line summarizing the central tendency of the distribution was created with a generalized additive model for integrated smoothness estimation.}}
\end{figure}

Figure \ref{BacklogRH} recreates here figures \citeyearpar[][237]{Licht2011} demonstrating the effect of different levels of legislative backlog\footnote{i.e. numers of directives pending approval} on directive deliberation time.

\begin{figure}
  \caption{Simulated Relative Hazards for the Effect of Different Levels of Legislative Backlog on Directive Deliberation Time}
  \caption{BacklogRH}

\end{figure}

\subsection{Common time-period specific biases}

The examples we have looked at so far study phenomenon using generic time.\footnote{Nonetheless, they may still be succeptiable to time-period specific biases, though these would be difficult to detect without expanding the sampling time frame.}

\section{Conclusion}

In this brief paper I have combined a number of important insights about how to build proportional hazards models into a simple worlkflow that will help researchers avoid common causes of model misspecification. An important insight is that the multiplicative interactions and time-period specific biases researchers should look out for in their models may actually be factors political scientists are interested in studying. So we need tools to fully explore them, not just ways to control for them. Doing this is often difficult in current software. So, I have also created and demonstrated a new R package--\emph{simPH}--that makes it considerably easier to effectively explore and present results from these models. In particular I  especially results that are the result of complex multiplicative interactions. 

\emph{simPH} is in the early stages of development, but more is on the way. In particular, it will be useful to add the ability to show results from interactions between different types of variables. For example, plotting quantities of interest from an interaction between a linear and spline fitted variable. The package would also be more useful if future versions were capable of simulating and plotting results for models with frailties, i.e. random effects \cite[see][]{BoxSteffensmeier2006}. Finally, I hope to expand the package so that it is an effective tool for presenting results from other proportional hazards event history analysis models, notably the Fine and Gray competing risks model.

\section*{Appendix}

\subsection*{Installing \emph{simPH}}

\emph{simPH} is currently available for download from GitHub.\footnote{\url{https://github.com/}} To install it into R type the following code into your R Console:

<<eval=FALSE>>=
devtools::install_github("simPH", "christophergandrud")
@

\noindent You need to have the \emph{devtools} package \citep{R-devtools} set up to install \emph{simPH}.

For more information about \emph{simPH} and see: \url{http://christophergandrud.github.com/simPH/}. Please report any bugs to \url{https://github.com/christophergandrud/simPH/issues}. Also please feel free to contribute to its development by making a pull request.

\subsection*{Source Code}

To help you implement the functions in \emph{simPH} I've included the R source code I used to create the figures in this article. 

\subsection*{Figure \ref{Spline1}}

<<Spline1Code, eval=FALSE, tidy=FALSE>>=
# Load packages
library(survival)
library(simPH)

# Load Carpenter (2002) data 
# The data is included with simPH
data("CarpenterFdaData")

# Run basic model
# From Keele (2010) replication data. Used to create Table 7.
M1 <- coxph(Surv(acttime, censor) ~  prevgenx + lethal + deathrt1 +
              acutediz + hosp01 + pspline(hospdisc, df = 4) + 
              pspline(hhosleng, df = 4) + mandiz01 + 
              femdiz01 + peddiz01 + orphdum + natreg + vandavg3 + 
              wpnoavg3 + pspline(condavg3, df = 4) + 
              pspline(orderent, df = 4) + pspline(stafcder, df = 4), 
              data = CarpenterFdaData)

# Simulated Fitted Values of stafcder
## stafcder is the number of FDA drug review staff
Sim1 <- coxsimSpline(M1, bspline = "pspline(stafcder, df = 4)", 
                     bdata = CarpenterFdaData$stafcder,
                     qi = "Hazard Ratio",
                     Xj = seq(1100, 1700, by = 10), 
                     Xl = seq(1099, 1699, by = 10), ci = "90")

# Plot simulated Hazard Ratios
ggspline(Sim1, qi = "Hazard Ratio", 
         xlab = "\nNumber of FDA Drug Review Staff ", palpha = 0.2)
@

\subsection*{Figure \ref{TermPlot}}

<<TermPlot, eval=FALSE, tidy=FALSE>>=
# Create termplot for stafcder
termplot(M1, term = 17, se = TRUE, rug = TRUE, ylab = "Log Hazard", 
         xlab = "Number of FDA Drug Review Staff")
@

\subsection*{Figure \ref{TVCQMV}}

<<TVCQMVCode, eval=FALSE>>=
# Load Golub & Steunenberg (2007) data
## The data is included with simPH
data("GolubEUPData")

# Create natural log time interactions
Golubtvc <- function(x){
  assign(paste0("l", x), tvc(GolubEUPData, b = x, tvar = "end", tfun = "log"))
}
GolubEUPData$Lcoop <-Golubtvc("coop")
GolubEUPData$Lqmv <- Golubtvc("qmv")
GolubEUPData$Lbacklog <- Golubtvc("backlog")
GolubEUPData$Lcodec <- Golubtvc("codec")
GolubEUPData$Lqmvpostsea <- Golubtvc("qmvpostsea")
GolubEUPData$Lthatcher <- Golubtvc("thatcher")

# Run Cox PH Model
M2 <- coxph(Surv(begin, end, event) ~
            qmv + qmvpostsea + qmvpostteu +
            coop + codec + eu9 + eu10 + eu12 +
            eu15 + thatcher + agenda + backlog +
            Lqmv + Lqmvpostsea + Lcoop + Lcodec +
            Lthatcher + Lbacklog,
         data = GolubEUPData,
         ties = "efron")

# Create simtvc object for Relative Hazard
Sim2 <- coxsimtvc(obj = M2, b = "qmv", btvc = "Lqmv",
                   qi = "First Difference",
                   tfun = "log", from = 80, to = 2000,
                   by = 15, ci = "95")

# Create relative hazard plot
ggtvc(Sim2, qi = "First Difference", xlab = "\nTime in Days")
@
% Bibliography
\bibliographystyle{apsr}
\bibliography{HRBibliography,HRPackages}

\end{document}





