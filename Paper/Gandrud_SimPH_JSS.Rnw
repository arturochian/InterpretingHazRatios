%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% simPH: Showing Estimates from Interactive and Nonlinear Cox Proportional Hazard Models
% Christopher Gandrud
% 25 January 2014
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[article]{jss}
\usepackage{amsmath}

<<include=FALSE>>=
#### Load Packages ####
library(repmis)

Packages <- c("car", "knitr", "gridExtra", "repmis", "devtools",
            "MASS", "simPH", "SPIn", "stats", "survival", "Zelig", "ggplot2")

LoadandCite(Packages, file = "HRPackages.bib")

#### Load data ####
# Load Carpenter (2002) data 
## The data is included with simPH
data("CarpenterFdaData")

# Load Golub & Steunenberg (2007) data
## The data is included with simPH
data("GolubEUPData")

options(prompt = "R>  ")

##### Set Chunk Options ####
opts_chunk$set(fig.align='center', dev='png', prompt=TRUE, highlight=FALSE, background="white")
@

% Author/Title
\author{Christopher Gandrud \\ Hertie School of Governance}
\title{\pkg{simPH}: An \proglang{R} Package for Showing Estimates for Interactive and Nonlinear Effects from Cox Proportional Hazard Models}

\Plainauthor{Christopher Gandrud}
\Plaintitle{simPH: An R Package for Showing Estimates for Interactive and Nonlinear Effects from Cox Proportional Hazard Models}
\Shorttitle{\pkg{simPH}}

% Abstract
\Abstract{
    The \proglang{R} package \pkg{simPH} provides tools for effectively communicating results from Cox Proportional Hazard (PH) models, especially models with interactive and nonlinear effects. The Cox Proportional Hazard model is a popular tool for examining cross-unit cross-time data. However, many researchers misspecify their models and poorly communicate uncertainty about their estimates. This is unfortunate because causes of model misspecification--e.g., interactive and nonlinear effects--may be substantively meaningful. Uncertainty about these effects can be difficult to assess because quantities of interest are often on asymmetric and nonlinear scales. Part of the problem has been that available computational tools make it difficult to explore and communicate these effects. \pkg{simPH} overcomes these problems by making it easy for researchers to simulate quantities of interest for a variety of effects estimated from Cox PH models and plot them. The package can also be used for simple linear effects, making it a useful all-round package for showing results from Cox PH models. \pkg{simPH}'s plots employ visual weighting in order to effectively communicate estimation uncertainty. The user also has the option of showing either the standard central interval of the simulation's distribution or the shortest probability interval--which can be especially useful for asymmetrically distributed estimates. I use hypothetical and empirical examples to illustrate \pkg{simPH}'s syntax and capabilities.
}

\Keywords{Cox Proportional Hazard models, hazard ratios, , time-varying, nonlinearity, splines, visual-weighting, \proglang{R}}
\Plainkeywords{Cox Proportional Hazard models, hazard ratios, time-interactions, time-varying, nonlinearity, splines, visual-weighting, R}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

\Address{
    Christopher Gandrud \\
    Hertie School of Governance \\
    Friedrichstrasse 180 \\
    Berlin, 10117 \\
    Germany \\
    E-mail: \email{gandrud@hertie-school.org}
}

\begin{document}

The \cite{cox1972} Proportional Hazards (PH) model is used in a wide range of disciplines including epidemiology and political science. However, many researchers misspecify their models and poorly communicate uncertainty about their estimates. This is unfortunate because causes of model misspecification--e.g., interactive and nonlinear effects--may be substantively meaningful. Uncertainty about these effects can be difficult to assess because quantities of interest are often on asymmetric and nonlinear scales. Part of the problem has been that available computational tools make it difficult to explore and communicate these effects. 

This article aims to improve how researchers use Cox-type models in two ways. After briefly discussing previous research on how Cox-type models can be misspecified by ignoring time interactions and nonlinear effects, it (a) advocates using shortest probability intervals and visual-weighting to display simulated quantities of interest that describe these estimated effects. Then (b) it makes it easy for researchers to use these methods by demonstrating the new \proglang{R} \citep{CiteR} package \pkg{simPH} \citep{R-simPH}, that is freely available on the Comprehensive \proglang{R} Archive Network. The package makes it easy to simulate distributions of quantities of interest estimated from Cox-type models and display the central or shortest probability intervals of these distributions using visually-weighted plots. The latter type of interval is often more appropriate for Cox-type model results. Quantities of interest \pkg{simPH} can handle include hazard ratios, first differences, relative hazards, marginal effects, and hazard rates from linear, linear-interactive, time-varying, and nonlinear coefficients. I use hypothetical and empirical examples to illustrate \pkg{simPH}'s syntax and capabilities. 

%%%%%% Section: Common issues
\section[The Cox PH model]{The Cox PH model}

Before discussing model misspecification problems, let's quickly look at what Cox PH models are. The Cox PH model is a semi-parametric survival model that allows researchers to examine how specified factors influence the rate of a particular event happening--e.g., infection, death, the adoption of a public policy--at a particular point in time given that the event has not already occurred. This rate is commonly referred to as the hazard rate ($h_{i}(t)$). The hazard rate for unit $i$ at time $t$ is estimated with the Cox PH model using: 
%
\begin{equation}
    h(t|\mathbf{X}_{i})=h_{0}(t)\mathrm{e}^{(\mathbf{\beta X}_{i})},
\end{equation}
%
where $h_{0}(t)$ is the baseline hazard, i.e., the instantaneous rate of a transition at time $t$ when all of the covariates are zero. $\mathbf{\beta}$ is a vector of coefficients and $\mathbf{X}_{i}$ is the vector of covariates for unit $i$.

We are often interested in how a covariate changes the rate of an event happening. In general researchers have tried to address this by looking at Cox PH coefficient estimates $\beta$. However, only examining single coefficients can lead to significant model misspecification and erroneous substantive interpretation of Cox PH results.

\section[Nonproportional and time-interactive]{Nonproportional hazards and time-interactive effects}

One of the most important sources of estimation bias in Cox PH models discussed at length by \cite{Licht2011}, \cite{BoxSteffensmeier2001}, and \cite{boxsteffensmeier2004} is a violation of the proportional hazards assumption (PHA). The PHA is that the hazards of two units experiencing an event are proportional to one another and that this relationship is constant over time. Formally, for the PHA to hold the hazard for units $j$ and $l$ must be:\footnote{This is also the equation for the hazard ratio between $x_{j}$ and $x_{l}$.}
%
\begin{equation}
	\frac{h_{j}(t)}{h_{l}(t)} = \mathrm{e}^{\beta\prime(x_{j} - x_{l})}.
\end{equation}
%
for all points in time. If the proportional hazards assumption is violated and measures are not taken to correct for the violation, then researchers may create biased parameter estimates and statistical tests with lower power \citep{Therneau1990,Keele2010}. Beyond these statistical problems, not adjusting for violations of the PHA can prevent researchers from finding evidence for phenomenon they are interested in studying, including how an effect changes over time and whether or not it changes nonlinearly over the range of a variable's values.

There are a number of widely used tests to examine if the PHA has been violated. See \cite{Grambsch1994}, \cite{BoxSteffensmeier2001}, and \cite{boxsteffensmeier2004} for discussions of various PHA testing methods.\footnote{Many software packages implement versions of these tests. \proglang{R}'s \pkg{survival} package \citep{R-survival} implements Grambsch and Therneau's \citeyearpar{Grambsch1994} modified Schoenfeld residuals test. This is done with the \code{cox.zph} command.} If a covariate is determined to violate the PHA, Box-Steffensmeier and co-authors \citep[see][]{BoxSteffensmeier2003,boxsteffensmeier2004} suggest directly modeling the relationship between the variable and time. This usually entails including an interaction between the variable and some function of time such as the natural logarithm or some exponent. The decision to use a particular functional form should be guided by theory and will likely also be influenced by findings in the data. If $f(t)$ is some function of time then a simple Nonproportional Hazards (NPH) Cox model estimating the hazard rate for unit $i$ with one time-interaction is given by:
%
\begin{equation}
	h_{i}(t|\mathbf{x}_{i})=h_{0}(t)\mathrm{e}^{(\beta_{1}x_{i} + \beta_{2}f(t)x_{i})}.
\end{equation}

Like any other interaction effect \cite[see][]{Brambor2006} extra care should be taken when interpreting the $\beta_{1}$ and $\beta_{2}$ parameter estimates and their associated uncertainty. We cannot simply interpret the effect by looking at $\beta_{1}$ or $\beta_{2}$ in isolation. They need to be combined. \cite{Licht2011} argues that post-estimation simulation techniques should be employed to substantively interpret these combined coefficients and the uncertainty surrounding them. Let's briefly look at ways to calculate combined effects. Later in this section we will look at showing our uncertainty about them using simulation techniques. 

\cite{Licht2011} describes two methods for calculating the combined effect of a time interaction on the hazard of an event happening in ways that are relatively easy to interpret: (a) first differences and (b) relative hazards. A first difference is the percentage change in the hazard rate at time $t$ between two values of $x$:
%
\begin{equation}
	\%\triangle h_{i}(t) = (e^{(x_{j} - x_{l})(\beta_{1} + \beta_{2}f(t))} - 1) * 100.
\end{equation}
% 
Relative hazards are given by:
%
\begin{equation}
	\frac{h_{j}(t)}{h_{l}(t)} = \mathrm{e}^{x_{j}(\beta_{1} + \beta_{2}f(t))}.
\end{equation}
%
In this situation the covariate $x_{l}$ is 0. Relative hazards represent the change in the hazard when $x$ is `switched on'. As such, relative hazards are a special case of the hazard ratio \citep[231]{Licht2011}, i.e., the expected change in the hazard when $x$ is fitted at a value different from zero compared to when $x$ is zero.
 
\subsection[Nonproportional and nonlinear effects]{Nonproportional hazards and nonlinear effects}

Time-interactive effects are not the only cause of PHA violations. Building on \cite{Grambsch1994} and \cite{Therneau2000}, \cite{Keele2010} points out that common diagnostic tests will also indicate PHA violations if the model is misspecified for other reasons. Omitting an important covariate, using a proportional hazards model even if another survival model is more appropriate, or including a covariate as linear when its effect is actually nonlinear can lead to significant PHA tests.\footnote{Note: addressing the omission of important covariates is beyond the scope of the \pkg{simPH.} package.} 

Because of this Keele suggests that \emph{before} testing the PHA we should try to make sure that we are not omitting important variables and find the covariates' appropriate functional forms, typically using either polynomials or splines.\footnote{See \cite{Keele2008} for a review of methods for identifying nonlinear effects and different spline types.} He demonstrates this in replication studies by adding penalized splines then using a Wald test to examine if the spline estimates have a better fit than their linear counterparts. Many studies using Cox PH models do not test for nonlinearity, but instead jump straight to testing the PHA, including time-interactions when it is violated. As Keele \citeyearpar{Keele2010} demonstrates, ascribing a time-interactive effect to a covariate when in fact the effect varies not over time, but nonlinearly over values of the covariate can have major implications for the substantive interpretation of results.

\section[Show estimates]{Show your estimation uncertainty}

How can we effectively examine and communicate both the point estimates of and our uncertainty about quantities of interest from time-interactive and nonlinear effects? In this section I advocate showing these results using simulations, shortest probability intervals, and visually-weighted plots.

\subsection{Post estimation simulations}

Following \cite{King2000}, \cite{Licht2011} proposes post-estimation simulation techniques to make it easier to estimate the uncertainty surrounding quantities of interest for time interactions like first differences and relative hazards.\footnote{See \citet[352-353]{King2000} for a discussion of alternative approaches including fully Bayesian Markov-Chain Monte Carlo techniques and bootstraping. The main difference between these three approaches is how the parameters are drawn.} In both cases we first find the parameter point estimates for $\hat{\beta_{1}}$ and $\hat{\beta_{2}}$ from a NPH Cox model that make up the time-varying effect as well as the parameter covariance estimates. Second, we draw $n$ values of $\beta_{1}$ and $\beta_{2}$ from the multivariate normal distribution with a mean of $\hat{\beta}$ and variance specified by the parameters' estimated covariance. Third, we use these simulated values to calculate a quantity of interest such as the first difference or relative hazard for a range of times as well as specified values of $x_{j}$ and $x_{l}$ (as appropriate). Finally, we plot the results. Using this simulation technique allows us to estimate full time-varying effects, how they change over time, substantively asses the effects, and show the uncertainty surrounding the estimates.

We can easily extend this simulation technique to quantities of interest for other estimated effect types, including nonlinear effects. For example if a nonlinear effect is modeled with a second order polynomial--i.e., $\beta_{1}x_{i} + \beta_{2}x_{i}^{2}$--we can once again draw $n$ simulations from the multivariate normal distribution for both $\beta_{1}$ and $\beta_{2}$. Then we simply calculate quantities of interest for a range of values and plot the results as before. For example, we find the first difference for a second order polynomial with:
%
\begin{equation}
    \%\triangle h_{i}(t) = (\mathrm{e}^{\beta_{1}x_{j-l} + \beta_{2}x_{j-l}^{2}} - 1) * 100,
\end{equation}
%
\noindent where $x_{j-l} = x_{j} - x_{l}$. Note we will not be showing the estimated effect over time.\footnote{For this we need to estimate the hazard rate.} but for a range of comparisons between $x_{j}$ and $x_{l}$.

We can use a similar procedure for splines. Penalized splines \citep{Eilers1996} are a commonly used way of showing more complex nonlinear effects than polynomials \cite[see][]{Keele2008}. They involve ``linear combinations of B-spline basis functions'' \citep[5]{Strasak2009} joined at points in the range of observed values of $x$ called ``knots'' \citep[50]{Keele2008}. A Cox PH model with one penalized spline is given by:
%
\begin{equation}
    h(t|\mathbf{X}_{i})=h_{0}(t)\mathrm{e}^{g(x)},
\end{equation}
%
where $g(x)$ is the penalized spline function. For our post-estimation purposes $g(x)$ is basically a series of linearly combined coefficients such that:
%
\begin{equation}
    g(x) = \beta_{k_{1}}(x)_{1+} + \beta_{k_{2}}(x)_{2+} + \beta_{k_{3}}(x)_{3+} + \ldots + \beta_{k_{n}}(x)_{n+},
\end{equation}  
%
where $k$ are the equally spaced spline knots with values inside of the range of observed $x$ and $n$ is the number of knots. $x_{c+}$ indicates that:
%
\begin{equation}
    (x)_{c+} = 
    \left \{
    \begin{array}{ll}
        x & \quad \text{if} \: k_{c-1} < x \leq k_{c} \\
        x & \quad \text{if} \: x \leq k_{1} \: \text{and} \: k_{c} = k_{1} \\
        x & \quad \text{if} \: x \geq k_{n} \: \text{and} \: k_{c} = k_{n} \\
        0 & \quad \text{otherwise.}
    \end{array}
    \right.
\end{equation}
%
Note, $x$ should be within the observed data. 

We can again draw values of each $\beta_{k_{1}}, \ldots \beta_{k_{n}}$ from the multivariate normal distribution described above. We then use these simulated coefficients to estimate quantities of interest for a range of covariate values. For example, the first difference between two values $x_{j}$ and $x_{l}$ is:
%
\begin{equation}
    \%\triangle h_{i}(t) = (\mathrm{e}^{g(x_{j}) - g(x_{l})} - 1) * 100.
\end{equation}
%
Relative hazards and hazard ratios can be calculated by extension. Once we find the simulated quantities of interest, plotting the results is straightforward. 

We can use this post-estimation simulation technique for virtually any quantity of interest estimated from Cox-type models including marginal effects for interactions \cite[see][]{Brambor2006} and plain linear effects. 

\subsection{Which interval to show?}

If researchers go beyond the usual `train timetable' coefficient tables and graphically show their parameter estimates--as \pkg{simPH} makes easier--they usually do so by plotting lines of some measure of central tendency and confidence bands calculated from standard errors over a range of fitted values. Previous work with post-estimation simulations, e.g., \cite{Licht2011}, has mirrored this approach in graphs with a line for the median or mean of the simulation distribution and lines representing the boundaries of a central interval of the distribution. For example, the central 95 percent interval could be represented by lines at the 2.5 and 97.5 percentiles of the distribution.

Many quantities of interest from Cox-type models have asymmetric probability distributions on a nonlinear scale and can therefore be poorly summarized by central intervals. Recall that most quantities of interest are on an exponential scale with a lower bound of 0 or, in the case of first differences, -100. They can have very long and sparse upper regions relative to a tighter concentration of the distribution near the lower boundary. In these cases it can be more useful to look at highest density regions \citep[see][]{Box1973,Hyndman1996}. The underlying idea is that we should be more interested in the set of quantities of interest values with the most probability \cite[120]{Hyndman1996}, rather than an arbitrary central interval. When the simulation has a normal distribution, the highest density region will be equivalent to the central interval with the same percentage of the simulations, e.g., 95 percent. However, when the highest density is at the boundary--for example when many of the simulated relative hazard values are close to 0--then \cite{Liu2013} argue that the highest density region is preferable to the central interval. In these cases ``central intervals can be much longer and have the awkward property [of] cutting off a narrow high-posterior slice that happens to be near the boundary, thus ruling out a part of the distribution that is actually strongly supported by the inference'' \citep[2]{Liu2013}. If this happens \citeauthor{Liu2013} recommend finding the shortest probability interval (SPIn). This is the shortest interval of a particular probability coverage based on the simulations. They find this to be a very efficient way of finding the shortest highest density region for unimodal distributions.

\subsection{Visual weighting}

Whether graphing a central or shortest probability interval, only using lines to represent the center and edges can draw the reader's attention away from what the graph is trying to communicate. This approach overemphasizes the edges of the interval, the areas of lowest probability. Some graphs uniformly shade the interval between the upper and lower bounds. Uniform shading suggest to the reader a uniform distribution between the edges. Both of these characteristics give misleading information about the quantities of interests' probability distributions, especially when they are on an exponential scale.

Visual weighting presents a solution to these problems. Hsiang calls visual weight ``the amount of a viewer's attention that a graphical object or display region attracts, based on visual properties'' \citeyearpar[3]{Hsiang2012}. More visual weight can be created by using more ``graphical ink" \citep{Tufte2001}. Visual weight is decreased by removing graphical ink. The simplest way to increase or decrease graphical ink with our simulations is to simply plot each simulation value as a point or series of values with a line that is semi-transparent. Areas of the distribution with many simulations will be darker. Areas with fewer simulations, often near the edges of the distribution, will be lighter. Plotting semi-transparent points or lines allows a researcher to very clearly communicate a quantity of interest's probability distribution. When each point or line is partially transparent areas of the chart where the points or lines are darker indicate areas of the distribution with higher probability as more points are stacked on top of one another. This approach gives more visual weight to areas of higher probability and avoids drawing the reader's attention to the edges of the distribution--the areas of lower probability--in the way that traditional confidence interval lines do. 

As a practical issue if a plot shows very many simulations as individual points, and to a lesser extent lines, it may create a very large file size. This is especially true if a researcher wants to use higher quality vector graphics. An alternative it to summarize simulated distributions with multiple ribbons of increasing transparency the further from the central tendency a portion of the distribution is. These ribbons could stretch across given segments of the distribution such as the central 50 and 95 percentage intervals. Using ribbons rather than points conveys somewhat less information about a distribution, but can be convenient if very many simulations are run.

%%%%%% Section: simPH
\section[simPH: Tools]{\pkg{simPH}: Tools for simulating and graphing effects}

One reason that researchers have inconsistently incorporated suggestions to test for and examine time interactions and nonlinearities in their Cox-type models and show their estimation uncertainty is that there has been a lack of computational capabilities to easily do so. In \proglang{R} the \pkg{survival} \citep{R-survival} package has functions for testing the proportional hazards assumption. The \pkg{survival} and \pkg{Zelig} \citep{R-zelig} packages can estimate models with splines and interactions. However, their capabilities for graphically showing these estimates and associated uncertainty are very limited. \pkg{Zelig} can plot basic estimates from Cox PH models using simulation techniques, but not time-interactive or nonlinear spline effects. Current capabilities for showing results from splines present results in difficult to interpret quantities and without indicating estimation uncertainty in a way that is easy to interpret. In general the capabilities for showing results from time interactions is very limited. Usually, showing these types of results requires considerable researcher effort to extract estimates from model objects and then devise ways to show them graphically. See for example Licht's \citeyearpar{Licht2011} \proglang{Stata} \citep{StataCite} code for replicating the time-interaction plots in her paper.\footnote{It is available at: \url{http://hdl.handle.net/1902.1/15633}.} No method allows you to easily plot shortest probability intervals and virtually none effectively uses visual weighting.

To solve these problems I am introducing the \pkg{simPH} package for \proglang{R}. There are three basic steps to use the package:

\begin{enumerate}
	\item Estimate a Cox-type model using \pkg{survival}'s \code{coxph} command.
	\item Simulate parameter estimates and calculate quantities of interest--e.g., relative hazards, first differences, hazard ratios, marginal effects for linear interactions, or hazard rates--using the function from the  \pkg{simPH} package corresponding to the variable type. See Table~\ref{simTable} for a summary of the simulation commands.
	\item Plot the simulations using the \code{simGG} method.
\end{enumerate}


\begin{table}
    \caption{\pkg{simPH} quantity of interest simulation commands}
    \label{simTable}
    \vspace{0.3cm}
    \begin{center}
        \begin{tabular}{l c}
            \hline
            Simulation command & Effect type \\
            \hline\hline
            \code{coxsimLinear} & linear \\
            \code{coxsimInteract} & linear multiplicative interactions \\
            \code{coxsimPoly} & polynomials \\
            \code{coxsimtvc} & time-interactive \\
            \code{coxsimSpline} & penalized splines \\
            \hline
        \end{tabular}
    \end{center}
\end{table}

\pkg{simPH}'s simulation functions follow \cite{King2000} to simulate parameters\footnote{\pkg{simPH} uses the \pkg{MASS} \citep{R-MASS} \proglang{R} package to draw the simulations.} and calculate a variety of quantities of interest. The user can specify the number of simulations to run per fitted value with the \code{nsim} argument. The default is 1000. The more simulations conducted, the better picture we get of the probability distributions our parameters are from \citep[349]{King2000}.\footnote{Warning: in some cases, especially with penalized splines, it is easy to ask the program to create more simulations than average desktop computers can easily handle. Therefore the user may need to balance a desire for a clear view of the probability distribution a quantity of interest comes from with what is computationally feasible.} \pkg{simPH}'s simulation commands allow the user to keep either the traditional central interval of the simulations' distributions--the middle 95 percent by default--or use the shortest probability interval--also 95 percent by default. In either case the range is set with the \code{ci} argument, i.e., to find the central 90 percent interval use \code{ci = 0.9}. To tell \pkg{simPH} to find the shortest probability interval with any of \pkg{simPH}'s' simulation commands simply set the argument \code{spin = TRUE}.\footnote{This capability was developed from \cite{Liu2013} and the accompanying code in Liu's \citeyearpar{R-SPIn} \pkg{SPIn} \proglang{R} package. It is currently unavailable for hazard rates.} 

The \code{simGG} plotting method can then be used to plot these simulated values as semi-transparent points. The transparency level can be set with the argument \code{alpha}. Furthermore a smoothing line of a type that can be specified by the user with the \code{smoother} argument is plotted to summarize the distribution's central tendency. In general, any smoothing method accepted by \pkg{ggplot2} \citep{R-ggplot2} can be used. By plotting semi-transparent points for each simulated quantity of interest value or lines for the series of values from each simulation, \code{simGG} automatically visually weights simulation distributions.

To choose between points, lines, or ribbons use the \code{type} argument and set it to equal \code{points}, \code{lines}, or \code{ribbons}, respectively. The default is \code{type = 'points'}. Ribbon plots will have three ribbons. The most transparent shows the furthest extent of the central or shortest probability interval. The less transparent ribbon shows the central 50 percent of this interval. And the middle line shows the interval's median.

The \code{simGG} method draws on \pkg{ggplot2} to plot the simulations. In most cases \code{simGG} returns a \pkg{ggplot2} \code{gg} object. As such you can add any aesthetic attributes to the plots that \pkg{ggplot2} will allow.

%%%%%% Section: Demonstrations
\section[Demonstrations]{Demonstrations}

To illustrate \pkg{simPH}'s syntax and capabilities we will first examine a simple example without interactive or nonlinear effects. Then we will replicate key figures and findings from \cite{Licht2011} and \cite{Keele2010}. The quantities of interest from these studies are from time-interactive and penalized spline effects. For examples with other quantities of interest and variable types please see \pkg{simPH}'s accompanying documentation. 

\subsection{A simple non-interactive and linear example}

\pkg{simPH} can be used to show results from effects that are not explicitly modeled as interactions or nonlinear effects using the \code{coxsimLinear} function. Let's look at an example using a hypothetical data set called \code{hmohiv} from \cite{hosmer2008}.  The data is hosted by and the basic estimation model is from \cite{UCLAtest}. The data set contains 100 members of a Health Maintenance Organization (HMO). All of the members are HIV positive. The HMO wants to examine their survival times. Note: the model we will estimate below does not have proportional hazard assumption violations.

Let's estimate a simple Cox PH model that includes information on the members' ages and intravenous drug use. We can load the necessary packages for this example and download the data with the following code:

<<LinearModel1.1, tidy=FALSE, echo=TRUE, message=FALSE, warning=FALSE>>=
library("survival") 
library("simPH")
hmohiv <- read.table(
            "http://www.ats.ucla.edu/stat/r/examples/asa/hmohiv.csv", 
            sep = ",", header = TRUE)
@

\noindent In the data set intravenous drug use is recorded in a simple dummy variable called \code{drug}, where one is a history of use and zero otherwise. Each member's age in years is recorded in a variable called \code{age}. The ages range from 20 to 54 with a median of 35. 

We are going to present results for how age is associated with survival times. Remember that quantities of interest such as relative hazards and hazard ratios are effectively comparisons. Relative hazards are simply hazard ratios comparing a unit with a zero value on some variable to another unit with another value of this variable. For the \code{age} variable this would mean a comparison between someone with an age of zero and someone with some other age. This may not be a particularly relevant comparison. It is also an out of sample comparison as the youngest HMO member is 20 years old. We can easily change the comparisons such that the median age (35) is the reference value:

<<LinearModel1.2, tidy=FALSE, echo=TRUE, message=FALSE, warning=FALSE>>=
hmohiv$AgeMed <- hmohiv$age - 35 
@ 

\noindent The new variable ranges from -15 to 19. Now all of the simulated relative hazards will be based on a comparison with a 35 year old. Let's estimate the model using the \code{coxph} command from the \pkg{survival} package:

<<LinearModel1.3, tidy=FALSE, echo=TRUE, message=FALSE, warning=FALSE>>=
M1 <- coxph(Surv(time, censor) ~ AgeMed + drug,  
        method = "breslow", data = hmohiv)
@

Age is clearly a time-varying quantity. Not only does it vary over time in the way a person's income does, for example, but in effect it has a linear interaction with time. However, in our model we do not explicitly interact age with time as we will in later examples. Therefore we can treat the variable in the same way as personal income for the purposes of creating simulations with \pkg{simPH}. So, we use the \code{coxsimLinear} command:

<<LinearModel1.4, tidy=FALSE, echo=TRUE, message=FALSE, warning=FALSE>>=
Sim1 <- coxsimLinear(M1, b = "AgeMed", Xj = seq(-15, 19, by = 0.2), 
                     qi = "Hazard Ratio")
@   

\noindent We told \code{coxsimLinear} that we wanted to simulate hazard ratios based on our model object \code{M1}. We specify which variable to find hazard ratios for using the \code{b} argument. The \code{Xj} argument lets us set the fitted values of $x_{j}$; in this case a sequence of values between -15 and 19 at intervals of 0.2. The smaller the interval, the smoother the resulting graph will look if you use points to plot the simulations. However, this will also increase the computation time required to run the simulations and plot the results. In addition to reducing the number of simulation values with \code{nsim}, increasing the sequence increment is an easy way to speed up processing times.

We have not specified the \code{Xl} argument our hazard ratios. In this case \code{coxsimLinear} automatically sets all $x_{l}$ to zero. To graphically present the results now in the \code{Sim1} object we can use the \code{simGG} method. The following code creates Figure~\ref{LinearPlot}.

<<LinearModel1.5, eval=FALSE, tidy=FALSE, echo=TRUE, message=FALSE, warning=FALSE>>=
simGG(Sim1, xlab = "\nYears of Age from the Sample Median (35)", 
      ylab = "Hazard Ratio with Comparison\n to a 35 Year Old\n")
@

\begin{figure}

<<LinearModel1.6, tidy=FALSE, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, fig.width=7, fig.height=4, out.width='0.6\\linewidth'>>=
simGG(Sim1, xlab = "\nYears of Age from the Sample Median (35)", 
      ylab = "Hazard Ratio with Comparison\n to a 35 Year Old\n",
      alpha = 0.05, type = 'lines')
@

    \caption{Simulated hazard ratios of age on survival time for HIV positive HMO members.}
    \label{LinearPlot}
\end{figure}

\noindent In Figure~\ref{LinearPlot} we can see that the hazard ratio at \code{AgeMed} zero (age 35) is one. This is because a hazard ratio for the value of a unit compared to the same value is always one. The simulated hazard ratios for ages below the median are less than one. This means that they are less likely to die at a given point in time than someone aged 35.\footnote{Because hazard ratios are bounded by zero it may be helpful to split the graph in two. One graph could range from -15 to zero in order to show detail in the predicted effect across this range.} Ages above the median have a hazard ratio greater than one and are therefore more likely to die than 35 year olds. We can see in this figure we have estimated that a 54 year old (those 19 years older than the median) are about six times more likely than 35 year olds to die at a given point in time, all else equal, though there is a considerable range of uncertainty around this point estimate.      

Three quick notes on syntax before moving on. The \code{xlab} and \code{ylab} arguments in \code{simGG} allowed us to set the x-axis and y-axis labels respectively. The backslash followed by the letter \code{n} creates a new line in the output.


\subsection{Time-interactive effects}

Let's now look at how to use \pkg{simPH} to explore time-interactive effects that are explicitly created by interacting a given variable with some time transformation in the estimation model. To do this we will recreate plots from \cite{Licht2011}. She re-examines Golub and Steunenberg's \citeyearpar{Golub2007} analysis of European Union legislative deliberation time. They wanted to find out what factors affected the amount of time it took the European Union to pass a new piece of legislation. Key variables they examined included the voting procedure that was used for a given piece of legislation--including the so-called qualified majority vote (QMV) procedure\footnote{The procedure changed over time, but essentially QMV requires some voting majority greater than 50\% + 1. See the European Union's website for more details: \url{http://europa.eu/legislation_summaries/glossary/qualified_majority_en.htm} (accessed January 2014).}--and the amount of other legislation pending, i.e. legislative backlog. Both of these variables violated the proportional hazard assumption and are more accurately modeled with log-time interactions.

The left panel of Figure~\ref{TVCQMV} recreates Licht's \citeyearpar{Licht2011} figure showing the first difference of a log-time interaction for how the effect of the QMV procedure on legislative deliberation time changes as the number of days of deliberation increases \citep[see][236]{Licht2011}.\footnote{Her original figure was not in terms of a percentage difference to make it more comparable to a figure in Golub and Steunenberg's original. I present the results in terms of percentage difference, as the first difference is commonly reported. I have not separated out the pre and post Single European Act time periods for simplicity. There are slight discrepancies in the estimates presented in her figures and those here. These are caused by differences in how the underlying model was estimated in \proglang{Stata} compared to \proglang{R}. Finally, I also examined whether or not nonlinearity functional forms would be better fits than time interactions as per our discussion above. However, I found no evidence of this.} To create this figure we first load the data \code{GolubEUPData}. This data set is included with \pkg{simPH}.

<<TVCModel1.1, tidy=FALSE, echo=TRUE, message=FALSE, warning=FALSE>>=
data("GolubEUPData")
@

Then we create the log-time interactions. Notice the use of the \code{tvc} command to create the log-time interactions. The command is included with the \pkg{simPH} package. The \code{data} argument let's us specify the data frame where our covariate and time variables are. The covariate we wish to interact is specified with \code{b} and the time variable with \code{tvar}. Finally, we specify the function of time with the \code{tfun} argument. The additional creation of the \code{Golubtvc} function is not necessary, it just makes the code tidier. 

<<TVCModel1.2, tidy=FALSE, echo=TRUE, message=FALSE, warning=FALSE>>=
Golubtvc <- function(x){
  tvc(data = GolubEUPData, b = x, tvar = "end", tfun = "log")
}

GolubEUPData$Lcoop <- Golubtvc("coop")
GolubEUPData$Lqmv <- Golubtvc("qmv")
GolubEUPData$Lbacklog <- Golubtvc("backlog")
GolubEUPData$Lcodec <- Golubtvc("codec")
GolubEUPData$Lqmvpostsea <- Golubtvc("qmvpostsea")
GolubEUPData$Lthatcher <- Golubtvc("thatcher")
@

Finally, estimate the model with \code{coxph}. Note that each log-time interaction is entered into the model along with the corresponding non-time interacted term.


<<TVCModel1.3, tidy=FALSE, echo=TRUE, message=FALSE, warning=FALSE>>=
M2 <- coxph(Surv(begin, end, event) ~ qmv + qmvpostsea + qmvpostteu +
              coop + codec + eu9 + eu10 + eu12 + eu15 + thatcher + 
              agenda + backlog + Lqmv + Lqmvpostsea + Lcoop + Lcodec +
              Lthatcher + Lbacklog,
            data = GolubEUPData, ties = "efron")
@

In the following code chunk we take the \code{M2} model object created by \code{coxph} and use it in \pkg{simPH}'s \code{coxsimtvc} function to simulate the first difference of the time-interactive effect of qualified majority voting (\code{qmv}) on directive deliberation time from 80 through 2000 days after deliberation begins. Again, we specify the quantity of interest that we want to simulate with the \code{qi} argument. The non-time interacted term is entered with the \code{b} argument as before. The time interaction term is entered with \code{btvc}. The form of the time interaction is declared with the \code{tfun} argument, i.e \code{tfun = "log"}.\footnote{Other options include \code{"linear"} for linear time-interactions and \code{"pow"} for polynomials. If \code{tfun = "pow"} then also set the argument \code{pow} to specify the power the time interaction was raised to.} \code{Xj = 1} specifies that the difference is between values of \code{qmv} 1 and 0, i.e., QMV was used or not. Again we could also use the \code{Xl} argument to change the comparison to a value other than 0. The \code{from} and \code{to} arguments allow you to specify the time period over which to simulate the quantity of interest. The \code{by} argument allows you to specify the increment of the time sequence to simulate values at. 

<<TVCModel2.1, eval=FALSE, tidy=FALSE>>=
Sim2 <- coxsimtvc(obj = M2, b = "qmv", btvc = "Lqmv",
                  qi = "First Difference", Xj = 1, tfun = "log", 
                  from = 80, to = 2000, by = 5)
@

Once we have created the \code{Sim2} object containing the simulations, we can simply use the \code{simGG} method to plot the results. In this example, we adjusted the median line size with the \code{lsize = 0.5} argument. This makes this particular plot clearer. Finally, the \code{legend = FALSE} argument hides the legend describing the fitted value comparisons that are being made between $x_{j}$ and $x_{l}$. A legend would be fairly uninformative in this case as we are only comparing the values 1 and 0.\footnote{The default is to show the legend. The \code{legend} argument follows the \pkg{ggplot2} syntax for creating plot guides, so to show the legend use \code{legend = "legend"}.} 

<<TVCModel2.2, eval=FALSE, tidy=FALSE>>=
simGG(Sim2, xlab = "\nTime in Days", title = "Central Interval\n", 
      type = "ribbons", lsize = 0.5, legend = FALSE, alpha = 0.3) 
@

We can clearly see in the left panel of Figure~\ref{TVCQMV}\footnote{The figures' ribbons extend across the middle or shortest probability 95 percent interval of the simulations. \\ As in Licht's \citeyearpar{Licht2011} original the time period plotted is truncated from 80 to 2000 to make the estimates more easily interpretable.} that QMV increases the probability of passing a directive early in the deliberation process (almost by 300 percent at about 80 days). But as the deliberation process goes on, the effect decreases and then becomes negative at about 1000 days. Finally, the rate of decrease levels off after 1000 days relative to before. 

The right-panel of Figure~\ref{TVCQMV} shows the shortest-probability interval for QMV simulated from \code{M2}. To create this we used all of the same code as above, except \code{spin = TRUE} was added to the \code{coxsimtvc} call. Notice that in this case, the central and shortest probability intervals are largely equivalent. Also because \code{simGG} returns \pkg{ggplot2} \code{gg} model objects, we can use \code{grid.arrange} from the \pkg{gridExtra} package \citep{R-gridExtra} to combine the two plots into one figure.\footnote{If we placed the two plots created by \code{simGG} into their own objects called \code{Plot1} and \code{Plot2} then we could combine them into one figure using \code{gridExtra::grid.arrange(Plot1, Plot2, ncol = 2)}.}

\begin{figure}

<<TVCModel3, tidy=FALSE, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, fig.width=7, fig.height=4, out.width='0.95\\linewidth', dev='pdf'>>=
library("ggplot2")
library("gridExtra")

## Create simtvc object for first difference (central interval)
Sim2.1 <- coxsimtvc(obj = M2, b = "qmv", btvc = "Lqmv",
                    qi = "First Difference", Xj = 1,
                    tfun = "log", from = 80, to = 2000,
                    by = 15, ci = 0.95)

# Create simtvc object for first difference (SPIn)
Sim2.2 <- coxsimtvc(obj = M2, b = "qmv", btvc = "Lqmv",
                    qi = "First Difference", Xj = 1,
                    tfun = "log", from = 80, to = 2000,
                    by = 15, ci = 0.95, spin = TRUE)


# Create first difference plots
Plot2.1 <- simGG(Sim2.1, xlab = "\nTime in Days", 
                 title = "Central Interval\n", alpha = 0.3,
                 type = "ribbons", lsize = 0.5, legend = FALSE)
                  
Plot2.2 <- simGG(Sim2.2, ylab = "", xlab = "\nTime in Days",
                 title = "SPIn\n", alpha = 0.3,
                 type = "ribbons", lsize = 0.5, legend = FALSE)

# Combine plots
grid.arrange(Plot2.1, Plot2.2, ncol = 2)
######### Save M2 #############
save(M2, file = "M2_ModelObject.RData")
@

    \caption{Simulated first differences for the effect of Qualified Majority Voting on the time it takes to pass legislation.}
    \label{TVCQMV}

\end{figure}

Figure~\ref{BacklogRH} plots simulated relative hazards for a variety of fitted values.\footnote{It replicates the right panel of Licht's \citeyearpar{Licht2011} Figure 3 \citeyearpar[][237]{Licht2011}. One difference is that she estimates uncertainty from 10 draws of 1000 simulations, whereas Figure~\ref{BacklogRH} is based on one draw of 1000 simulations. The figure's ribbons extend across the middle 95 percent of the simulations.} This figure demonstrates the effect of different levels of legislative backlog (\code{backlog}) on directive deliberation time from 1200 to 2000 days after the directive was proposed. The effect shown is also modeled as a log-time interaction. The process for creating the plot is similar to what we have already seen. The only differences to note are that we entered a sequence of values for backlogged legislation for \code{Xj} in \code{coxsimtvc} ranging from 40 to 200 at increments of 40. This creates five separate sets of relative hazard simulations, one for each value of \code{Xj} compared to 0. Because this creates many simulations, we used ribbons rather than points to visually weight the simulated distributions. Finally, we specified the legend name for the plot in the \code{simGG} call with the \code{leg.name} argument.

<<TVCBacklogDataLoad, include=FALSE>>=
######### Load cached model object ############
load("M2_ModelObject.RData")
@

<<TVCBacklog1, echo=TRUE, tidy=FALSE, eval=FALSE>>=
Sim3 <- coxsimtvc(obj = M2, b = "backlog", btvc = "Lbacklog",
                  qi = "Relative Hazard", Xj = seq(40, 200, 40),
                  tfun = "log", from = 1200, to = 2000, by = 10,
                  nsim = 500)

simGG(Sim3, xlab = "\nTime in Days", type = "ribbons",
        leg.name = "Backlogged \n Items", alpha = 0.2)
@

\begin{figure}

<<TVCBacklog2, tidy=FALSE, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, fig.width=7, fig.height=4, out.width='0.95\\linewidth', dev='pdf'>>=
# Create simtvc object for relative hazard
Sim3 <- coxsimtvc(obj = M2, b = "backlog", btvc = "Lbacklog",
                  qi = "Relative Hazard", Xj = seq(40, 200, 40),
                  tfun = "log", from = 1200, to = 2000, by = 10,
                  nsim = 500)

# Create relative hazard plot
simGG(Sim3, xlab = "\nTime in Days", ribbons = TRUE,
        leg.name = "Backlogged \n Items", alpha = 0.2)
@
  \caption{Simulated relative hazards for the effect of different levels of legislative backlog on directive deliberation time.}
  \label{BacklogRH}
\end{figure}

The main conclusion we can draw from this presentation of the log-time interaction is that if a piece of legislation is not passed in the first 1200 or so days from when it was proposed, it is less likely that it will be passed if there is a large legislative backlog \cite[for more details see][236-237]{Licht2011}.  

In the previous examples, the simulation probability distributions are not strongly influenced by boundary effects or long upper tails. Because of this the central and shortest probability intervals are largely equivalent. The distributions are also relatively tight, which is indicated by fairly even visual weight across the distributions.

\subsection{Nonlinear effects: Penalized Splines}

Let's now turn to look at how \pkg{simPH} can be used to simulate and plot quantities of interest estimated from penalized splines by building on Keele's \citeyearpar{Keele2010} replication study. To demonstrate why researchers need to check for and explicitly model nonlinearity he replicated a study by \cite{Carpenter2002} on the time it takes the US Food and Drug Administration (FDA) to approve a new drug. Using the steps discussed above, he found that modeling nonlinearity with penalized splines rather than time-varying effects was the more appropriate strategy for dealing with covariates that violated the PHA.  This allowed him to draw conclusions that, for example, the number of FDA review staff increases the likelihood that a drug will be accepted, but that the effect diminishes after a threshold number of staff are assigned. 

It has been difficult to examine and communicate the functional form, magnitude, and uncertainty surrounding spline effects. Coefficient tables are very cumbersome, because a spline fitted effect is estimated using multiple coefficients and standard errors for values of a variable in a given range--demarcated by the knots--on the hazard. Depending on the size of the ranges\footnote{With R's \code{pspline} command the range can effectively be adjusted by changing the spline's degrees of freedom.} there can quickly be many more coefficients than can be efficiently presented in a table and understood by a reader. In his results tables, Keele does not show spline coefficients and simply denotes their overall significance with standard significance stars. 

In \proglang{R} you can plot estimated spline effects over a range of values with the \code{termplot} command. These plots, however, have a number of drawbacks. First, the plots show the log hazard ratio,\footnote{Log hazard ratios for a standard Cox PH model are given by: $\mathrm{log} \left\{\frac{h_{j}(t)}{h_{h}(t)}\right\} = \mathbf{\beta X}_{i}$ \cite[modified from][49]{boxsteffensmeier2004}.} which is not a particularly intuitive quantity to understand and rarely reported in studies using Cox PH. More importantly it plots standard errors, instead of the more widely used central confidence intervals. Casual readers could easily think the uncertainty around the spline estimates is smaller than it really is. The plots give no other information about the likely distribution of the estimated quantity of interest.

\pkg{simPH} allows us to estimate quantities that we are more interested in like relative hazards, hazard rates over time, hazard ratios, and first differences. For example, let's simulate the hazard ratios for the effect of an additional drug review staff on the time it takes for a drug to be approved. Figure~\ref{Spline1}\footnote{The figure's points show the middle 95 percent of the simulations at each value of FDA staff. The line summarizing the central tendency of the distribution was created with a generalized additive model for integrated smoothness estimation.} shows the simulated hazard ratios over the full range of FDA staff per drug trial (\code{stafcder}) observed in Carpenter's data. Not only is this more informative than simply showing the coefficients and their standard errors that comprise the spline parameter estimates, it is also more informative than the current plotting alternative in \proglang{R}.

Let's look at the syntax used to create Figure~\ref{Spline1}. Load Carpenter's data included with \pkg{simPH} and estimate the model with splines using \code{coxph} and \code{pspline}. The model was originally used to create the results in Keele's \citeyearpar{Keele2010} Table 7.

<<FitKeeleModel, include=TRUE, tidy=FALSE,>>=
data("CarpenterFdaData")

M3 <- coxph(Surv(acttime, censor) ~  prevgenx + lethal + deathrt1 +
              acutediz + hosp01 + pspline(hospdisc, df = 4) + 
              pspline(hhosleng, df = 4) + mandiz01 + 
              femdiz01 + peddiz01 + orphdum + natreg + vandavg3 + 
              wpnoavg3 + pspline(condavg3, df = 4) + 
              pspline(orderent, df = 4) + pspline(stafcder, df = 4), 
              data = CarpenterFdaData)
@

Now simulate the hazard ratios for a sequence of values with \code{coxsimSpline} and graph the results with \code{simGG}.

<<Spline1Fig1, eval=FALSE, tidy=FALSE>>=
Sim4 <- coxsimSpline(M3, bspline = "pspline(stafcder, df = 4)", 
                     bdata = CarpenterFdaData$stafcder,
                     qi = "Hazard Ratio",
                     Xj = seq(1100, 1700, by = 10), 
                     Xl = seq(1099, 1699, by = 10))

simGG(Sim4, xlab = "\n Number of FDA Drug Review Staff", 
        title = "Central Interval\n", alpha = 0.1)
@ 

There are a few syntax points to note. First we tell \code{coxsimSpline} the term to estimate for using the \code{bspline} argument and the same syntax we used to enter the term as a penalized spline in the \code{coxph} call, i.e., \code{pspline(stafcder, df = 4)}.\footnote{You need to make sure that there is a white space before and after the \code{=}.} Second, we specified the vector containing the observed \code{stafcder} data with the \code{bdata} argument. This is important for \code{coxsimSpline} to be able to accurately find the spline knots. 

\begin{figure}

<<Spline1Fig2, echo=FALSE, tidy=FALSE, message=FALSE, warning=FALSE, , fig.width=7, fig.height=4, out.width='0.6\\linewidth', cache=TRUE>>=
## Simulated Fitted Values
Sim4 <- coxsimSpline(M3, bspline = "pspline(stafcder, df = 4)", 
                     bdata = CarpenterFdaData$stafcder,
                     qi = "Hazard Ratio",
                     Xj = seq(1100, 1700, by = 10), 
                     Xl = seq(1099, 1699, by = 10))

# Plot simulated values
SimPlot1 <- simGG(Sim4, xlab = "\n Number of FDA Drug Review Staff", 
        title = "Central Interval\n", alpha = 0.1, type = "lines")

SimPlot1 + scale_y_continuous(breaks = c(0, 20, 40, 60), limits = c(0, 60))
@  

  \caption{Simulated hazard ratios for the effect of FDA staff on drug approval time. Showing the central 95\% interval.}
  \label{Spline1}

\end{figure}

We can see in Figure~\ref{Spline1} that the simulated values are concentrated near the bottom of the distribution.\footnote{Note that the simulation values have been smoothed using post-simulation cubic smoothing splines. Smoothing is accomplished with \code{smooth.spline}, which is part of basic \proglang{R}. This gives a more continuous appearance to the simulations. You can tell \code{simGG} to not smooth the simulations by setting the argument \code{SmoothSpline = FALSE}.} Following \citeauthor{Liu2013} we may find it useful to show not the central 95 percent interval, but the shortest 95 percent probability interval. Figure~\ref{Spline2} shows this interval.\footnote{The line summarizing the central tendency of the distribution was created with a generalized additive model for integrated smoothness estimation.} To create it we used all of the same syntax as before, while simply adding \code{spin = TRUE} to \code{coxsimSpline}. Using the shortest 95 percent probability interval indicates that there is actually a higher probability that the hazard ratio is 1--i.e., no effect--than the central 95 percent interval for all but the highest quartile or so of FDA drug review staff. It also deemphasizes the higher hazard ratio values, where there is a low concentration of the probability. In both figures \ref{Spline1} and \ref{Spline2} the visual weighting draws readers' attention to the lower part of the distribution where the model estimates there is a higher probability that the hazard ratio will be. 

Finally, to ease comparison, let's put the second plot on the same y-axis scale as the first. To do this imagine that we placed the output of \code{coxsimSpline} with the argument \code{spin = TRUE} into an object called \code{SimPlot1}. We can then use \pkg{ggplot2} to change the y-axis range so that it is comparable to Figure~\ref{Spline1}'s:

<<Spline2Fig1, eval=FALSE, tidy=FALSE>>=
library("ggplot2")

SimPlot1 + scale_y_continuous(breaks = c(0, 20, 40, 60), limits = c(0, 60))
@  

\begin{figure}

<<Spline2Fig2, echo=FALSE, tidy=FALSE, message=FALSE, , fig.width=7, fig.height=4, out.width='0.6\\linewidth', cache=TRUE>>=
# Simulated Fitted Values: shortest probability interval
Sim4 <- coxsimSpline(M3, bspline = "pspline(stafcder, df = 4)", 
                     bdata = CarpenterFdaData$stafcder,
                     qi = "Hazard Ratio",
                     Xj = seq(1100, 1700, by = 10), 
                     Xl = seq(1099, 1699, by = 10), 
                     spin = TRUE)

# Plot simulated values
SimPlot2 <- simGG(Sim4, xlab = "\n Number of FDA Drug Review Staff",
                title = "SPIn\n", alpha = 0.1, type = "lines")

library("ggplot2")

SimPlot2 + scale_y_continuous(breaks = c(0, 20, 40, 60), 
                              limits = c(0, 60))
@  

  \caption{Simulated hazard ratios for the effect of FDA staff on drug approval time. Showing the shortest 95\% probability interval.}
  \label{Spline2}
\end{figure}

\section[Conclusion]{Conclusion}

In this paper I have reviewed a number of important insights about avoiding Cox PH model misspecification. Looking out for and properly estimating the interactions and nonlinearities that can cause biased estimates is crucial for researchers using Cox Proportional Hazard models. Beyond creating biased estimates, they can actually be factors that we are interested in studying. Until now the tools for fully exploring them, including their associated uncertainty, have been lacking. The \pkg{Zelig} package for \proglang{R} comes closest to allowing uses to so estimation uncertainty from Cox Proportional Hazards models. However, it has limited or no ability to simulate quantities of interest for interactive and nonlinear estimated effects. So, this paper has demonstrated a new \proglang{R} package--\pkg{simPH}--that makes it considerably easier to effectively explore and present quantities of interest for interactive and nonlinear effects. It can also be used for simple linear effects, making it a useful all-round package for showing results from Cox Proportional Hazard models. The paper has also argued for and demonstrated the usefulness of visual weighting and shortest probability intervals for understanding Cox-type models. \pkg{simPH} fully supports these methods.

% Acknowledgments
\section*{Acknowledgments}
Thank you to Andreas Beger, Jeffrey Chwieroth, Kelly Kadera, Luke Keele, Alison Post, Meredith Wilf, participants of the International Studies Association Annual Convention (2013), and two anonymous reviewers. The paper can be completely reproduced from source code files available at: \url{https://github.com/christophergandrud/InterpretingHazRatios}.

% Bibliography
\bibliographystyle{jss}
\bibliography{HRBibliography,HRPackages}

\end{document}





